{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6329548,"sourceType":"datasetVersion","datasetId":3640718}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Importing Necessary Libraries\n\nimport os\nimport json\nimport random\nimport math\nimport re\nimport numpy as np\nimport sentencepiece as spm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport pandas as pd\nfrom collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.517543Z","iopub.execute_input":"2025-10-27T08:36:27.518114Z","iopub.status.idle":"2025-10-27T08:36:27.522267Z","shell.execute_reply.started":"2025-10-27T08:36:27.518091Z","shell.execute_reply":"2025-10-27T08:36:27.521511Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# URDU TEXT NORMALIZATION\n\ndef normalize_urdu_text(text):\n    \n    # Remove diacritics (Unicode range U+064B to U+065F and U+0670)\n    text = re.sub(r'[\\u064B-\\u065F\\u0670]', '', text)\n    \n    # Standardize Alef forms\n    text = re.sub(r'[آأإٱ]', 'ا', text)\n    \n    # Standardize Yeh forms\n    text = re.sub(r'ے', 'ی', text)\n    text = re.sub(r'ۓ', 'ی', text)\n    \n    # Remove extra whitespace\n    text = ' '.join(text.split())\n    \n    return text.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.560638Z","iopub.execute_input":"2025-10-27T08:36:27.561137Z","iopub.status.idle":"2025-10-27T08:36:27.565277Z","shell.execute_reply.started":"2025-10-27T08:36:27.561115Z","shell.execute_reply":"2025-10-27T08:36:27.564583Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# DATA PREPROCESSING\n\ndef preprocess_urdu_dataset(data_path, output_dir=\"preprocessed\"):\n    \n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(\"Loading dataset\")\n    df = pd.read_csv(data_path, sep=\"\\t\")\n    sentences = df[\"sentence\"].dropna().astype(str).tolist()\n    \n    # Normalize all sentences\n    print(\"Normalizing Urdu text\")\n    sentences = [normalize_urdu_text(s) for s in sentences]\n    sentences = [s for s in sentences if len(s.strip()) > 3]\n    print(f\"✅ Total sentences after normalization: {len(sentences)}\")\n    \n    # Create conversation pairs\n    print(\"Creating conversation pairs\")\n    pairs = []\n    for i in range(len(sentences) - 1):\n        src = sentences[i]\n        tgt = sentences[i + 1]\n        if len(src.split()) > 1 and len(tgt.split()) > 1:\n            pairs.append({\"src\": src, \"tgt\": tgt})\n    \n    print(f\"Created {len(pairs)} conversation pairs\")\n    \n    # Save all text for tokenizer training\n    corpus_path = os.path.join(output_dir, \"corpus.txt\")\n    with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n        for text in sentences:\n            f.write(text + \"\\n\")\n    \n    print(f\"Saved corpus to {corpus_path}\")\n    return pairs\n\ndef train_tokenizer(corpus_path, output_prefix, vocab_size=16000):\n    \"\"\"Train SentencePiece tokenizer for Urdu\"\"\"\n    print(f\"Training SentencePiece tokenizer (vocab_size={vocab_size})...\")\n    spm.SentencePieceTrainer.Train(\n        f\"--input={corpus_path} \"\n        f\"--model_prefix={output_prefix} \"\n        f\"--vocab_size={vocab_size} \"\n        f\"--model_type=bpe \"\n        f\"--character_coverage=1.0 \"\n        f\"--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3\"\n    )\n    print(f\"Tokenizer trained: {output_prefix}.model\")\n\ndef prepare_dataset(pairs, sp_model, output_dir):\n    \n    print(\"Loading tokenizer...\")\n    sp = spm.SentencePieceProcessor(model_file=sp_model)\n    \n    # Tokenize pairs\n    print(\"Tokenizing all pairs...\")\n    data = []\n    for pair in tqdm(pairs, desc=\"Tokenizing\"):\n        src_ids = sp.encode(pair[\"src\"], out_type=int)\n        tgt_ids = sp.encode(pair[\"tgt\"], out_type=int)\n        data.append({\n            \"src\": pair[\"src\"],\n            \"tgt\": pair[\"tgt\"],\n            \"src_ids\": src_ids,\n            \"tgt_ids\": tgt_ids\n        })\n    \n    # Shuffle and split (80/10/10)\n    random.shuffle(data)\n    n = len(data)\n    train_idx = int(0.8 * n)\n    val_idx = int(0.9 * n)\n    \n    splits = {\n        \"train\": data[:train_idx],\n        \"val\": data[train_idx:val_idx],\n        \"test\": data[val_idx:]\n    }\n    \n    print(f\"\\n📊 Dataset Split:\")\n    for split_name, split_data in splits.items():\n        path = os.path.join(output_dir, f\"{split_name}.jsonl\")\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            for item in split_data:\n                f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n        print(f\"  - {split_name}: {len(split_data)} samples ({len(split_data)/n*100:.1f}%)\")\n    \n    return splits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.638280Z","iopub.execute_input":"2025-10-27T08:36:27.638823Z","iopub.status.idle":"2025-10-27T08:36:27.648761Z","shell.execute_reply.started":"2025-10-27T08:36:27.638802Z","shell.execute_reply":"2025-10-27T08:36:27.647899Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# DATASET CLASS\n\nclass ConversationDataset(Dataset):\n    \"\"\"PyTorch Dataset for conversation pairs\"\"\"\n    def __init__(self, jsonl_path, max_len=64):\n        self.data = []\n        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                self.data.append(json.loads(line))\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        # Add BOS and EOS tokens\n        src_ids = [2] + item[\"src_ids\"][:self.max_len-2] + [3]\n        tgt_ids = [2] + item[\"tgt_ids\"][:self.max_len-2] + [3]\n        \n        # Pad sequences\n        src_len = len(src_ids)\n        tgt_len = len(tgt_ids)\n        \n        src_ids += [0] * (self.max_len - src_len)\n        tgt_ids += [0] * (self.max_len - tgt_len)\n        \n        return {\n            \"src\": torch.tensor(src_ids[:self.max_len], dtype=torch.long),\n            \"tgt\": torch.tensor(tgt_ids[:self.max_len], dtype=torch.long),\n            \"src_len\": min(src_len, self.max_len),\n            \"tgt_len\": min(tgt_len, self.max_len),\n            \"src_text\": item[\"src\"],\n            \"tgt_text\": item[\"tgt\"]\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.649949Z","iopub.execute_input":"2025-10-27T08:36:27.650292Z","iopub.status.idle":"2025-10-27T08:36:27.669259Z","shell.execute_reply.started":"2025-10-27T08:36:27.650274Z","shell.execute_reply":"2025-10-27T08:36:27.668573Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# TRANSFORMER COMPONENTS\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                            (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def split_heads(self, x):\n        batch_size = x.size(0)\n        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n        return x.transpose(1, 2)\n    \n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n        \n        # Linear projections\n        Q = self.split_heads(self.W_q(query))\n        K = self.split_heads(self.W_k(key))\n        V = self.split_heads(self.W_v(value))\n        \n        # Scaled dot-product attention\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n        \n        context = torch.matmul(attn_weights, V)\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        \n        return self.W_o(context)\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        # Self-attention with residual connection\n        attn_out = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_out))\n        \n        # Feed-forward with residual connection\n        ff_out = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_out))\n        \n        return x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n        # Self-attention with residual connection\n        attn_out = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_out))\n        \n        # Cross-attention with encoder output\n        attn_out = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_out))\n        \n        # Feed-forward with residual connection\n        ff_out = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_out))\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.680567Z","iopub.execute_input":"2025-10-27T08:36:27.681049Z","iopub.status.idle":"2025-10-27T08:36:27.695620Z","shell.execute_reply.started":"2025-10-27T08:36:27.681021Z","shell.execute_reply":"2025-10-27T08:36:27.694800Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# FULL TRANSFORMER MODEL\n\nclass TransformerSeq2Seq(nn.Module):\n    \"\"\"\n    Complete Transformer Encoder-Decoder Model (REQUIREMENT)\n    Built from scratch without pre-trained models\n    \"\"\"\n    def __init__(self, vocab_size, d_model=256, num_heads=4, \n                 num_encoder_layers=2, num_decoder_layers=2, \n                 d_ff=512, dropout=0.1, max_len=512):\n        super().__init__()\n        \n        self.d_model = d_model\n        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n        \n        # Encoder stack\n        self.encoder_layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_encoder_layers)\n        ])\n        \n        # Decoder stack\n        self.decoder_layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_decoder_layers)\n        ])\n        \n        self.output_layer = nn.Linear(d_model, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        \n    def create_padding_mask(self, seq):\n        \"\"\"Create mask for padding tokens\"\"\"\n        return (seq != 0).unsqueeze(1).unsqueeze(2)\n    \n    def create_look_ahead_mask(self, size):\n        \"\"\"Create causal mask for decoder\"\"\"\n        mask = torch.triu(torch.ones(size, size), diagonal=1).bool()\n        return ~mask\n    \n    def encode(self, src):\n        \"\"\"Encoder: Process full input context\"\"\"\n        src_mask = self.create_padding_mask(src)\n        x = self.embedding(src) * math.sqrt(self.d_model)\n        x = self.pos_encoding(x)\n        \n        for layer in self.encoder_layers:\n            x = layer(x, src_mask)\n        \n        return x\n    \n    def decode(self, tgt, enc_output, src_mask=None):\n        \"\"\"Decoder: Generate response token-by-token\"\"\"\n        tgt_mask = self.create_padding_mask(tgt)\n        look_ahead_mask = self.create_look_ahead_mask(tgt.size(1)).to(tgt.device)\n        combined_mask = tgt_mask & look_ahead_mask.unsqueeze(0)\n        \n        x = self.embedding(tgt) * math.sqrt(self.d_model)\n        x = self.pos_encoding(x)\n        \n        for layer in self.decoder_layers:\n            x = layer(x, enc_output, src_mask, combined_mask)\n        \n        return self.output_layer(x)\n    \n    def forward(self, src, tgt):\n        \"\"\"Forward pass for training\"\"\"\n        src_mask = self.create_padding_mask(src)\n        enc_output = self.encode(src)\n        output = self.decode(tgt[:, :-1], enc_output, src_mask)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.728631Z","iopub.execute_input":"2025-10-27T08:36:27.729132Z","iopub.status.idle":"2025-10-27T08:36:27.737632Z","shell.execute_reply.started":"2025-10-27T08:36:27.729112Z","shell.execute_reply":"2025-10-27T08:36:27.736966Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# TRAINING WITH TEACHER FORCING\n\ndef train_model(model, train_loader, val_loader, device, epochs=20, lr=3e-4, \n                teacher_forcing_ratio=0.5):\n    \n    model = model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n    \n    best_val_loss = float('inf')\n    training_history = []\n    \n    print(f\"🚀 Starting Training (Teacher Forcing Ratio: {teacher_forcing_ratio})\")\n    \n    for epoch in range(epochs):\n        # Training\n        model.train()\n        train_loss = 0\n        \n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        for batch in pbar:\n            src = batch[\"src\"].to(device)\n            tgt = batch[\"tgt\"].to(device)\n            \n            optimizer.zero_grad()\n            \n            # Teacher forcing\n            output = model(src, tgt)\n            \n            # Calculate loss\n            loss = criterion(output.reshape(-1, output.size(-1)), \n                           tgt[:, 1:].reshape(-1))\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            train_loss += loss.item()\n            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n        \n        avg_train_loss = train_loss / len(train_loader)\n        \n        # ===== VALIDATION PHASE =====\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                src = batch[\"src\"].to(device)\n                tgt = batch[\"tgt\"].to(device)\n                output = model(src, tgt)\n                loss = criterion(output.reshape(-1, output.size(-1)), \n                               tgt[:, 1:].reshape(-1))\n                val_loss += loss.item()\n        \n        avg_val_loss = val_loss / len(val_loader)\n        perplexity = math.exp(avg_val_loss)\n        \n        # Log metrics\n        metrics = {\n            'epoch': epoch + 1,\n            'train_loss': avg_train_loss,\n            'val_loss': avg_val_loss,\n            'perplexity': perplexity\n        }\n        training_history.append(metrics)\n        \n        print(f\"\\n📊 Epoch {epoch+1} Results:\")\n        print(f\"   Train Loss: {avg_train_loss:.4f}\")\n        print(f\"   Val Loss: {avg_val_loss:.4f}\")\n        print(f\"   Perplexity: {perplexity:.2f}\")\n        \n        # Save best model\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': avg_val_loss,\n                'perplexity': perplexity,\n                'training_history': training_history\n            }, 'best_model.pth')\n            print(f\"   ✅ Model saved! (Best Val Loss: {avg_val_loss:.4f})\")\n        print()\n    \n    return training_history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.738780Z","iopub.execute_input":"2025-10-27T08:36:27.739027Z","iopub.status.idle":"2025-10-27T08:36:27.755269Z","shell.execute_reply.started":"2025-10-27T08:36:27.739010Z","shell.execute_reply":"2025-10-27T08:36:27.754584Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# INFERENCE FUNCTIONS\n\ndef greedy_decode(model, src, max_len, device, bos_id=2, eos_id=3):\n   \n    model.eval()\n    src = src.to(device)\n    \n    with torch.no_grad():\n        enc_output = model.encode(src)\n        src_mask = model.create_padding_mask(src)\n        \n        tgt = torch.tensor([[bos_id]], dtype=torch.long, device=device)\n        \n        for _ in range(max_len):\n            output = model.decode(tgt, enc_output, src_mask)\n            next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)\n            tgt = torch.cat([tgt, next_token], dim=1)\n            \n            if next_token.item() == eos_id:\n                break\n        \n        return tgt[0].cpu().tolist()\n\ndef beam_search_decode(model, src, beam_size, max_len, device, bos_id=2, eos_id=3):\n    \n    model.eval()\n    src = src.to(device)\n    \n    with torch.no_grad():\n        enc_output = model.encode(src)\n        src_mask = model.create_padding_mask(src)\n        \n        # Initialize beam with BOS token\n        beams = [(torch.tensor([[bos_id]], dtype=torch.long, device=device), 0.0)]\n        completed = []\n        \n        for _ in range(max_len):\n            candidates = []\n            \n            for seq, score in beams:\n                if seq[0, -1].item() == eos_id:\n                    completed.append((seq, score))\n                    continue\n                \n                output = model.decode(seq, enc_output, src_mask)\n                log_probs = F.log_softmax(output[:, -1, :], dim=-1)\n                top_probs, top_indices = log_probs.topk(beam_size)\n                \n                for prob, idx in zip(top_probs[0], top_indices[0]):\n                    new_seq = torch.cat([seq, idx.unsqueeze(0).unsqueeze(0)], dim=1)\n                    new_score = score + prob.item()\n                    candidates.append((new_seq, new_score))\n            \n            if not candidates:\n                break\n            \n            # Select top beam_size candidates\n            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n            \n            if len(completed) >= beam_size:\n                break\n        \n        # Return best completed sequence\n        if completed:\n            best_seq = max(completed, key=lambda x: x[1])[0]\n        else:\n            best_seq = beams[0][0]\n        \n        return best_seq[0].cpu().tolist()\n\ndef generate_response(model, sp, input_text, max_len=64, device='cpu', \n                     strategy='greedy', beam_size=3):\n    \n    # Normalize input\n    input_text = normalize_urdu_text(input_text)\n    \n    # Encode input\n    src_ids = [2] + sp.encode(input_text, out_type=int) + [3]\n    src_tensor = torch.tensor([src_ids], dtype=torch.long)\n    \n    # Generate output\n    if strategy == 'beam':\n        output_ids = beam_search_decode(model, src_tensor, beam_size, max_len, device)\n    else:\n        output_ids = greedy_decode(model, src_tensor, max_len, device)\n    \n    # Decode output\n    response = sp.decode([id for id in output_ids if id not in [0, 2, 3]])\n    \n    return response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.756004Z","iopub.execute_input":"2025-10-27T08:36:27.756289Z","iopub.status.idle":"2025-10-27T08:36:27.774816Z","shell.execute_reply.started":"2025-10-27T08:36:27.756263Z","shell.execute_reply":"2025-10-27T08:36:27.774124Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# EVALUATION METRICS\n\ndef calculate_bleu_score(references, hypotheses):\n    \n    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n    \n    smooth = SmoothingFunction()\n    scores = []\n    \n    for ref, hyp in zip(references, hypotheses):\n        ref_tokens = [ref.split()]\n        hyp_tokens = hyp.split()\n        \n        try:\n            score = sentence_bleu(ref_tokens, hyp_tokens, \n                                smoothing_function=smooth.method1)\n            scores.append(score)\n        except:\n            scores.append(0.0)\n    \n    return np.mean(scores) if scores else 0.0\n\ndef calculate_rouge_l(references, hypotheses):\n    \n    try:\n        from rouge_score import rouge_scorer\n        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n        \n        scores = []\n        for ref, hyp in zip(references, hypotheses):\n            score = scorer.score(ref, hyp)\n            scores.append(score['rougeL'].fmeasure)\n        \n        return np.mean(scores) if scores else 0.0\n    except ImportError:\n        print(\"⚠️ rouge-score not installed. Install: pip install rouge-score\")\n        return 0.0\n\ndef calculate_chrf(references, hypotheses):\n    \n    def char_ngrams(text, n):\n        return [text[i:i+n] for i in range(len(text)-n+1)]\n    \n    scores = []\n    for ref, hyp in zip(references, hypotheses):\n        ref_chars = set(char_ngrams(ref, 3))\n        hyp_chars = set(char_ngrams(hyp, 3))\n        \n        if len(hyp_chars) == 0:\n            scores.append(0.0)\n            continue\n        \n        precision = len(ref_chars & hyp_chars) / len(hyp_chars)\n        recall = len(ref_chars & hyp_chars) / len(ref_chars) if len(ref_chars) > 0 else 0\n        \n        f_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n        scores.append(f_score)\n    \n    return np.mean(scores) if scores else 0.0\n\ndef evaluate_model(model, test_loader, sp, device, num_samples=100):\n    \n    print(\"EVALUATING MODEL\")\n    \n    model.eval()\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n    \n    references = []\n    hypotheses = []\n    total_loss = 0\n    \n    with torch.no_grad():\n        for i, batch in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n            if i >= num_samples // test_loader.batch_size:\n                break\n            \n            src = batch[\"src\"].to(device)\n            tgt = batch[\"tgt\"].to(device)\n            \n            # Calculate loss for perplexity\n            output = model(src, tgt)\n            loss = criterion(output.reshape(-1, output.size(-1)), \n                           tgt[:, 1:].reshape(-1))\n            total_loss += loss.item()\n            \n            # Generate predictions for BLEU/ROUGE/chrF\n            for j in range(min(4, src.size(0))):  # Evaluate 4 per batch\n                src_tensor = src[j:j+1]\n                pred_ids = greedy_decode(model, src_tensor, 64, device)\n                pred_text = sp.decode([id for id in pred_ids if id not in [0, 2, 3]])\n                \n                ref_text = batch[\"tgt_text\"][j]\n                \n                references.append(ref_text)\n                hypotheses.append(pred_text)\n    \n    # Calculate metrics\n    perplexity = math.exp(total_loss / len(test_loader))\n    bleu = calculate_bleu_score(references, hypotheses)\n    rouge_l = calculate_rouge_l(references, hypotheses)\n    chrf = calculate_chrf(references, hypotheses)\n    \n    metrics = {\n        'perplexity': perplexity,\n        'bleu': bleu,\n        'rouge_l': rouge_l,\n        'chrf': chrf\n    }\n    \n    print(f\"\\n{'='*60}\")\n    print(\"📊 EVALUATION RESULTS\")\n    print(f\"{'='*60}\")\n    print(f\"  Perplexity: {perplexity:.2f}\")\n    print(f\"  BLEU Score: {bleu:.4f}\")\n    print(f\"  ROUGE-L: {rouge_l:.4f}\")\n    print(f\"  chrF: {chrf:.4f}\")\n    print(f\"{'='*60}\\n\")\n    \n    # Show qualitative examples\n    print(\"🔍 QUALITATIVE EXAMPLES:\\n\")\n    for i in range(min(5, len(references))):\n        print(f\"Input: {references[i][:50]}...\")\n        print(f\"Predicted: {hypotheses[i]}\")\n        print(f\"Reference: {references[i]}\")\n        print(\"-\" * 60)\n    \n    # Save evaluation results\n    with open('evaluation_results.json', 'w', encoding='utf-8') as f:\n        json.dump({\n            'metrics': metrics,\n            'examples': [{'ref': r, 'hyp': h} for r, h in zip(references[:10], hypotheses[:10])]\n        }, f, ensure_ascii=False, indent=2)\n    \n    print(\"✅ Evaluation results saved to 'evaluation_results.json'\\n\")\n    \n    return metrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.775999Z","iopub.execute_input":"2025-10-27T08:36:27.776245Z","iopub.status.idle":"2025-10-27T08:36:27.796575Z","shell.execute_reply.started":"2025-10-27T08:36:27.776229Z","shell.execute_reply":"2025-10-27T08:36:27.795669Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# HUMAN EVALUATION FRAMEWORK\n\ndef conduct_human_evaluation(model, sp, device, test_samples=20):\n\n    evaluation_data = []\n    \n    test_inputs = [\n        \"آپ کیسے ہیں؟\",\n        \"آج موسم کیسا ہے؟\",\n        \"پاکستان کا دارالحکومت کیا ہے؟\",\n        \"میں آپ کی مدد کیسے کر سکتا ہوں؟\",\n        \"شکریہ\"\n    ]\n    \n    for i, input_text in enumerate(test_inputs[:test_samples], 1):\n        print(f\"\\n{'='*60}\")\n        print(f\"Sample {i}/{min(test_samples, len(test_inputs))}\")\n        print(f\"{'='*60}\")\n        \n        response = generate_response(model, sp, input_text, device=device)\n        \n        print(f\"\\nInput: {input_text}\")\n        print(f\"Generated Response: {response}\")\n        print()\n        \n        # In production, collect these from human evaluators\n        # For now, create a template\n        evaluation_data.append({\n            'input': input_text,\n            'response': response,\n            'fluency': None,  # 1-5 score\n            'relevance': None,  # 1-5 score\n            'adequacy': None  # 1-5 score\n        })\n    \n    # Save template for human evaluation\n    with open('human_evaluation_template.json', 'w', encoding='utf-8') as f:\n        json.dump(evaluation_data, f, ensure_ascii=False, indent=2)\n    \n    print(\"\\n✅ Human evaluation template saved to 'human_evaluation_template.json'\")\n    print(\"   Please fill in the fluency, relevance, and adequacy scores (1-5)\")\n    \n    return evaluation_data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.797388Z","iopub.execute_input":"2025-10-27T08:36:27.797580Z","iopub.status.idle":"2025-10-27T08:36:27.815960Z","shell.execute_reply.started":"2025-10-27T08:36:27.797565Z","shell.execute_reply":"2025-10-27T08:36:27.815207Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# MAIN EXECUTION\n\ndef main():\n    # CONFIGURATION\n    CONFIG = {\n        'data_path': \"/kaggle/input/urdu-dataset-20000/final_main_dataset.tsv\",\n        'output_dir': \"preprocessed\",\n        'vocab_size': 16000,\n        'd_model': 256,\n        'num_heads': 4,\n        'num_encoder_layers': 2,\n        'num_decoder_layers': 2,\n        'd_ff': 512,\n        'dropout': 0.2,\n        'batch_size': 32,\n        'epochs': 20,\n        'lr': 3e-4,\n        'max_len': 64,\n        'device': \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    }\n    \n    for key, value in CONFIG.items():\n        print(f\"  {key}: {value}\")\n    \n    print(f\"\\n  Device: {CONFIG['device']}\")\n    if CONFIG['device'] == 'cuda':\n        print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n    print()\n    \n    # PREPROCESSING\n    print(\"STEP 1: DATA PREPROCESSING\")\n    \n    pairs = preprocess_urdu_dataset(CONFIG['data_path'], CONFIG['output_dir'])\n    \n    # TOKENIZER TRAINING\n    print(\"STEP 2: TOKENIZER TRAINING\")\n    \n    train_tokenizer(\n        corpus_path=os.path.join(CONFIG['output_dir'], \"corpus.txt\"),\n        output_prefix=os.path.join(CONFIG['output_dir'], \"sp_urdu\"),\n        vocab_size=CONFIG['vocab_size']\n    )\n    \n    # Dataset Prepare\n    print(\"STEP 3: DATASET PREPARATION\")\n    \n    sp_model_path = os.path.join(CONFIG['output_dir'], \"sp_urdu.model\")\n    prepare_dataset(pairs, sp_model_path, CONFIG['output_dir'])\n    \n    # Data loading\n    print(\"STEP 4: CREATING DATALOADERS\")\n    \n    train_ds = ConversationDataset(\n        os.path.join(CONFIG['output_dir'], \"train.jsonl\"),\n        max_len=CONFIG['max_len']\n    )\n    val_ds = ConversationDataset(\n        os.path.join(CONFIG['output_dir'], \"val.jsonl\"),\n        max_len=CONFIG['max_len']\n    )\n    test_ds = ConversationDataset(\n        os.path.join(CONFIG['output_dir'], \"test.jsonl\"),\n        max_len=CONFIG['max_len']\n    )\n    \n    train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size'])\n    test_loader = DataLoader(test_ds, batch_size=CONFIG['batch_size'])\n    \n    print(f\"  Training batches: {len(train_loader)}\")\n    print(f\"  Validation batches: {len(val_loader)}\")\n    print(f\"  Test batches: {len(test_loader)}\")\n    \n    # Model Initialize\n    print(f\"\\n{'='*60}\")\n    print(\"STEP 5: MODEL INITIALIZATION\")\n    print(f\"{'='*60}\\n\")\n    \n    sp = spm.SentencePieceProcessor(model_file=sp_model_path)\n    vocab_size = sp.get_piece_size()\n    \n    model = TransformerSeq2Seq(\n        vocab_size=vocab_size,\n        d_model=CONFIG['d_model'],\n        num_heads=CONFIG['num_heads'],\n        num_encoder_layers=CONFIG['num_encoder_layers'],\n        num_decoder_layers=CONFIG['num_decoder_layers'],\n        d_ff=CONFIG['d_ff'],\n        dropout=CONFIG['dropout'],\n        max_len=512\n    )\n    \n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    print(f\"  Total Parameters: {total_params:,}\")\n    print(f\"  Trainable Parameters: {trainable_params:,}\")\n    print(f\"  Model Size: {total_params * 4 / 1024 / 1024:.2f} MB (float32)\")\n    \n    # Training\n    print(\"STEP 6: MODEL TRAINING\")\n    \n    training_history = train_model(\n        model, \n        train_loader, \n        val_loader, \n        CONFIG['device'],\n        epochs=CONFIG['epochs'],\n        lr=CONFIG['lr'],\n        teacher_forcing_ratio=0.5\n    )\n    \n    # Save training history\n    with open('training_history.json', 'w') as f:\n        json.dump(training_history, f, indent=2)\n    \n    # LOAD BEST MODEL\n    print(\"STEP 7: LOADING BEST MODEL\")\n    \n    checkpoint = torch.load('best_model.pth', map_location=CONFIG['device'])\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(f\" Loaded best model from epoch {checkpoint['epoch']+1}\")\n    print(f\"   Validation Loss: {checkpoint['val_loss']:.4f}\")\n    print(f\"   Perplexity: {checkpoint['perplexity']:.2f}\")\n    \n    # Evaluation\n    print(\"STEP 8: MODEL EVALUATION\")\n    \n    metrics = evaluate_model(model, test_loader, sp, CONFIG['device'], num_samples=100)\n    \n    # Testing\n    print(\"STEP 9: INTERACTIVE TESTING\")\n    \n    test_inputs = [\n        \"آپ کا نام کیا ہے؟\",\n        \"آج موسم کیسا ہے؟\",\n        \"پاکستان کے بارے میں بتائیں\",\n        \"شکریہ\",\n        \"خدا حافظ\",\n        \"میں آپ کی مدد کیسے کر سکتا ہوں؟\"\n    ]\n    \n    print(\"Testing with sample inputs:\\n\")\n    for inp in test_inputs:\n        print(f\"{'='*60}\")\n        print(f\"Input: {inp}\")\n        \n        # Greedy decoding\n        greedy_response = generate_response(\n            model, sp, inp, \n            device=CONFIG['device'], \n            strategy='greedy'\n        )\n        print(f\"Greedy Response: {greedy_response}\")\n        \n        # Beam search decoding\n        beam_response = generate_response(\n            model, sp, inp, \n            device=CONFIG['device'], \n            strategy='beam',\n            beam_size=3\n        )\n        print(f\"Beam Search Response: {beam_response}\")\n        print()\n    \n    # HUMAN EVALUATION SETUP\n    print(\"STEP 10: HUMAN EVALUATION SETUP\")\n    conduct_human_evaluation(model, sp, CONFIG['device'], test_samples=5)\n    \n    # Save Model\n    print(\"SAVING COMPLETE MODEL\")\n    \n    torch.save({\n        'config': CONFIG,\n        'model_state_dict': model.state_dict(),\n        'vocab_size': vocab_size,\n        'training_history': training_history,\n        'evaluation_metrics': metrics\n    }, 'complete_model.pth')\n    \n    # Summary\n\n    print(\"\\n Final Metrics:\")\n    print(f\"   - Perplexity: {metrics['perplexity']:.2f}\")\n    print(f\"   - BLEU: {metrics['bleu']:.4f}\")\n    print(f\"   - ROUGE-L: {metrics['rouge_l']:.4f}\")\n    print(f\"   - chrF: {metrics['chrf']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.816878Z","iopub.execute_input":"2025-10-27T08:36:27.817193Z","iopub.status.idle":"2025-10-27T08:36:27.835706Z","shell.execute_reply.started":"2025-10-27T08:36:27.817146Z","shell.execute_reply":"2025-10-27T08:36:27.835009Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# RUN MAIN\n\nif __name__ == \"__main__\":\n    # Set random seeds for reproducibility\n    random.seed(42)\n    np.random.seed(42)\n    torch.manual_seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n    \n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.836430Z","iopub.execute_input":"2025-10-27T08:36:27.836638Z","iopub.status.idle":"2025-10-27T08:41:21.999849Z","shell.execute_reply.started":"2025-10-27T08:36:27.836622Z","shell.execute_reply":"2025-10-27T08:41:21.999289Z"}},"outputs":[{"name":"stdout","text":"  data_path: /kaggle/input/urdu-dataset-20000/final_main_dataset.tsv\n  output_dir: preprocessed\n  vocab_size: 16000\n  d_model: 256\n  num_heads: 4\n  num_encoder_layers: 2\n  num_decoder_layers: 2\n  d_ff: 512\n  dropout: 0.2\n  batch_size: 32\n  epochs: 20\n  lr: 0.0003\n  max_len: 64\n  device: cuda\n\n  Device: cuda\n  GPU: Tesla P100-PCIE-16GB\n\nSTEP 1: DATA PREPROCESSING\nLoading dataset\nNormalizing Urdu text\n✅ Total sentences after normalization: 19979\nCreating conversation pairs\nCreated 19310 conversation pairs\nSaved corpus to preprocessed/corpus.txt\nSTEP 2: TOKENIZER TRAINING\nTraining SentencePiece tokenizer (vocab_size=16000)...\nTokenizer trained: preprocessed/sp_urdu.model\nSTEP 3: DATASET PREPARATION\nLoading tokenizer...\nTokenizing all pairs...\n","output_type":"stream"},{"name":"stderr","text":"Tokenizing: 100%|██████████| 19310/19310 [00:00<00:00, 20736.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Dataset Split:\n  - train: 15448 samples (80.0%)\n  - val: 1931 samples (10.0%)\n  - test: 1931 samples (10.0%)\nSTEP 4: CREATING DATALOADERS\n  Training batches: 483\n  Validation batches: 61\n  Test batches: 61\n\n============================================================\nSTEP 5: MODEL INITIALIZATION\n============================================================\n\n  Total Parameters: 10,843,776\n  Trainable Parameters: 10,843,776\n  Model Size: 41.37 MB (float32)\nSTEP 6: MODEL TRAINING\n🚀 Starting Training (Teacher Forcing Ratio: 0.5)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/20: 100%|██████████| 483/483 [00:13<00:00, 35.42it/s, loss=6.2857]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 1 Results:\n   Train Loss: 6.5703\n   Val Loss: 6.1026\n   Perplexity: 447.01\n   ✅ Model saved! (Best Val Loss: 6.1026)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 483/483 [00:13<00:00, 35.42it/s, loss=6.1280]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 2 Results:\n   Train Loss: 5.8215\n   Val Loss: 5.7691\n   Perplexity: 320.26\n   ✅ Model saved! (Best Val Loss: 5.7691)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 483/483 [00:13<00:00, 35.46it/s, loss=5.5364]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 3 Results:\n   Train Loss: 5.4313\n   Val Loss: 5.5939\n   Perplexity: 268.79\n   ✅ Model saved! (Best Val Loss: 5.5939)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 483/483 [00:13<00:00, 35.40it/s, loss=5.4471]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 4 Results:\n   Train Loss: 5.1359\n   Val Loss: 5.3975\n   Perplexity: 220.85\n   ✅ Model saved! (Best Val Loss: 5.3975)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 483/483 [00:13<00:00, 35.22it/s, loss=5.0505]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 5 Results:\n   Train Loss: 4.8960\n   Val Loss: 5.2642\n   Perplexity: 193.28\n   ✅ Model saved! (Best Val Loss: 5.2642)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 100%|██████████| 483/483 [00:13<00:00, 35.29it/s, loss=4.5092]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 6 Results:\n   Train Loss: 4.6900\n   Val Loss: 5.1516\n   Perplexity: 172.70\n   ✅ Model saved! (Best Val Loss: 5.1516)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|██████████| 483/483 [00:13<00:00, 35.36it/s, loss=4.5522]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 7 Results:\n   Train Loss: 4.5079\n   Val Loss: 5.0341\n   Perplexity: 153.56\n   ✅ Model saved! (Best Val Loss: 5.0341)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|██████████| 483/483 [00:13<00:00, 35.54it/s, loss=4.6771]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 8 Results:\n   Train Loss: 4.3460\n   Val Loss: 4.9486\n   Perplexity: 140.97\n   ✅ Model saved! (Best Val Loss: 4.9486)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/20: 100%|██████████| 483/483 [00:13<00:00, 35.55it/s, loss=4.1521]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 9 Results:\n   Train Loss: 4.1985\n   Val Loss: 4.8633\n   Perplexity: 129.45\n   ✅ Model saved! (Best Val Loss: 4.8633)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/20: 100%|██████████| 483/483 [00:13<00:00, 35.54it/s, loss=4.1079]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 10 Results:\n   Train Loss: 4.0631\n   Val Loss: 4.7906\n   Perplexity: 120.37\n   ✅ Model saved! (Best Val Loss: 4.7906)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/20: 100%|██████████| 483/483 [00:13<00:00, 35.57it/s, loss=4.0965]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 11 Results:\n   Train Loss: 3.9391\n   Val Loss: 4.7011\n   Perplexity: 110.07\n   ✅ Model saved! (Best Val Loss: 4.7011)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 100%|██████████| 483/483 [00:13<00:00, 35.62it/s, loss=3.8738]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 12 Results:\n   Train Loss: 3.8222\n   Val Loss: 4.6635\n   Perplexity: 106.00\n   ✅ Model saved! (Best Val Loss: 4.6635)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/20: 100%|██████████| 483/483 [00:13<00:00, 35.35it/s, loss=3.9045]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 13 Results:\n   Train Loss: 3.7116\n   Val Loss: 4.6236\n   Perplexity: 101.86\n   ✅ Model saved! (Best Val Loss: 4.6236)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/20: 100%|██████████| 483/483 [00:13<00:00, 34.95it/s, loss=3.8084]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 14 Results:\n   Train Loss: 3.6157\n   Val Loss: 4.5562\n   Perplexity: 95.22\n   ✅ Model saved! (Best Val Loss: 4.5562)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/20: 100%|██████████| 483/483 [00:13<00:00, 35.38it/s, loss=3.5970]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 15 Results:\n   Train Loss: 3.5123\n   Val Loss: 4.4819\n   Perplexity: 88.40\n   ✅ Model saved! (Best Val Loss: 4.4819)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/20: 100%|██████████| 483/483 [00:13<00:00, 35.25it/s, loss=3.3389]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 16 Results:\n   Train Loss: 3.4214\n   Val Loss: 4.4707\n   Perplexity: 87.42\n   ✅ Model saved! (Best Val Loss: 4.4707)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/20: 100%|██████████| 483/483 [00:13<00:00, 35.52it/s, loss=3.3299]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 17 Results:\n   Train Loss: 3.3380\n   Val Loss: 4.4177\n   Perplexity: 82.91\n   ✅ Model saved! (Best Val Loss: 4.4177)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/20: 100%|██████████| 483/483 [00:13<00:00, 35.42it/s, loss=3.2765]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 18 Results:\n   Train Loss: 3.2546\n   Val Loss: 4.3806\n   Perplexity: 79.88\n   ✅ Model saved! (Best Val Loss: 4.3806)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/20: 100%|██████████| 483/483 [00:13<00:00, 35.43it/s, loss=3.4271]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 19 Results:\n   Train Loss: 3.1836\n   Val Loss: 4.3564\n   Perplexity: 77.97\n   ✅ Model saved! (Best Val Loss: 4.3564)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/20: 100%|██████████| 483/483 [00:13<00:00, 35.47it/s, loss=3.3447]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 20 Results:\n   Train Loss: 3.1029\n   Val Loss: 4.3094\n   Perplexity: 74.40\n   ✅ Model saved! (Best Val Loss: 4.3094)\n\nSTEP 7: LOADING BEST MODEL\n Loaded best model from epoch 20\n   Validation Loss: 4.3094\n   Perplexity: 74.40\nSTEP 8: MODEL EVALUATION\nEVALUATING MODEL\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:   5%|▍         | 3/61 [00:00<00:07,  8.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"⚠️ rouge-score not installed. Install: pip install rouge-score\n\n============================================================\n📊 EVALUATION RESULTS\n============================================================\n  Perplexity: 1.24\n  BLEU Score: 0.0053\n  ROUGE-L: 0.0000\n  chrF: 0.0448\n============================================================\n\n🔍 QUALITATIVE EXAMPLES:\n\nInput: کالی کافی اور سگاروں پہ گزارا کرتی۔...\nPredicted: اس کی خلاف ورزی پر غور کیا ہی؟\nReference: کالی کافی اور سگاروں پہ گزارا کرتی۔\n------------------------------------------------------------\nInput: کی سن پیدائش کی سب سی ہٹ فلم...\nPredicted: اس سی زیادہ کیا جا رہا ہی؟\nReference: کی سن پیدائش کی سب سی ہٹ فلم\n------------------------------------------------------------\nInput: جب سیاست دانوں کی ساتھ عوام کا عملی تعلق ہو گا۔...\nPredicted: میں نی اپنی طرف دیکھ لیا\nReference: جب سیاست دانوں کی ساتھ عوام کا عملی تعلق ہو گا۔\n------------------------------------------------------------\nInput: ہماری تاریخ میں اس کی بی شمار مثالیں مو جود ہیں۔...\nPredicted: یہ کوئی نہیں کہا جائی گا کہ فلاں ادمی دائرۂ اسلام سی خارج کررہی ہیں\nReference: ہماری تاریخ میں اس کی بی شمار مثالیں مو جود ہیں۔\n------------------------------------------------------------\nInput: یہ صورتحال دیکھ کر کلیجہ منہ کو انی لگتا ہی...\nPredicted: وہ اپنی بندوں پر پوری طرح حاوی ہی\nReference: یہ صورتحال دیکھ کر کلیجہ منہ کو انی لگتا ہی\n------------------------------------------------------------\n✅ Evaluation results saved to 'evaluation_results.json'\n\nSTEP 9: INTERACTIVE TESTING\nTesting with sample inputs:\n\n============================================================\nInput: آپ کا نام کیا ہے؟\nGreedy Response: ایک اور سننی والی مسلط ہی۔\nBeam Search Response: اس کا حصہ ہی۔\n\n============================================================\nInput: آج موسم کیسا ہے؟\nGreedy Response: ان کی وجہ سی ایک ہی تو اس میں سوال کیی تو ایک ہی ۔\nBeam Search Response: ان کی وجہ سی ایک ہی\n\n============================================================\nInput: پاکستان کے بارے میں بتائیں\nGreedy Response: ایک بار پھر میرا امتحان شروع ہو گئی ہیں\nBeam Search Response: جہاں دو متحارب افواج ایک دوسری کی مد مقابل ہوں۔\n\n============================================================\nInput: شکریہ\nGreedy Response: اج ان کی طرف سی بڑا اچھا ہی\nBeam Search Response: اج ان کی طرف سی بڑا اچھا ہی\n\n============================================================\nInput: خدا حافظ\nGreedy Response: تو پھر مفاد پرستی کی انتہا پسندی کا درجہ حرارت\nBeam Search Response: تو پھر مفاد کی انتہا ہی۔\n\n============================================================\nInput: میں آپ کی مدد کیسے کر سکتا ہوں؟\nGreedy Response: یہ بات ہی۔\nBeam Search Response: یہ بات ہی۔\n\nSTEP 10: HUMAN EVALUATION SETUP\n\n============================================================\nSample 1/5\n============================================================\n\nInput: آپ کیسے ہیں؟\nGenerated Response: یہ مقدمہ اخلاقی شعور کی گا۔\n\n\n============================================================\nSample 2/5\n============================================================\n\nInput: آج موسم کیسا ہے؟\nGenerated Response: ان کی وجہ سی ایک ہی تو اس میں سوال کیی تو ایک ہی ۔\n\n\n============================================================\nSample 3/5\n============================================================\n\nInput: پاکستان کا دارالحکومت کیا ہے؟\nGenerated Response: اس سی وہ امن اور انصاف کا بڑا چیلنج ہی۔\n\n\n============================================================\nSample 4/5\n============================================================\n\nInput: میں آپ کی مدد کیسے کر سکتا ہوں؟\nGenerated Response: یہ بات ہی۔\n\n\n============================================================\nSample 5/5\n============================================================\n\nInput: شکریہ\nGenerated Response: اج ان کی طرف سی بڑا اچھا ہی\n\n\n✅ Human evaluation template saved to 'human_evaluation_template.json'\n   Please fill in the fluency, relevance, and adequacy scores (1-5)\nSAVING COMPLETE MODEL\n\n Final Metrics:\n   - Perplexity: 1.24\n   - BLEU: 0.0053\n   - ROUGE-L: 0.0000\n   - chrF: 0.0448\n","output_type":"stream"}],"execution_count":26}]}