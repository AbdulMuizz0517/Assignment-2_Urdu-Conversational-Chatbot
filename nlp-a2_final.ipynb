{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6329548,"sourceType":"datasetVersion","datasetId":3640718}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Importing Necessary Libraries\n\nimport os\nimport json\nimport random\nimport math\nimport re\nimport numpy as np\nimport sentencepiece as spm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport pandas as pd\nfrom collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.517543Z","iopub.execute_input":"2025-10-27T08:36:27.518114Z","iopub.status.idle":"2025-10-27T08:36:27.522267Z","shell.execute_reply.started":"2025-10-27T08:36:27.518091Z","shell.execute_reply":"2025-10-27T08:36:27.521511Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# URDU TEXT NORMALIZATION\n\ndef normalize_urdu_text(text):\n    \n    # Remove diacritics (Unicode range U+064B to U+065F and U+0670)\n    text = re.sub(r'[\\u064B-\\u065F\\u0670]', '', text)\n    \n    # Standardize Alef forms\n    text = re.sub(r'[ÿ¢ÿ£ÿ•Ÿ±]', 'ÿß', text)\n    \n    # Standardize Yeh forms\n    text = re.sub(r'€í', '€å', text)\n    text = re.sub(r'€ì', '€å', text)\n    \n    # Remove extra whitespace\n    text = ' '.join(text.split())\n    \n    return text.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.560638Z","iopub.execute_input":"2025-10-27T08:36:27.561137Z","iopub.status.idle":"2025-10-27T08:36:27.565277Z","shell.execute_reply.started":"2025-10-27T08:36:27.561115Z","shell.execute_reply":"2025-10-27T08:36:27.564583Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# DATA PREPROCESSING\n\ndef preprocess_urdu_dataset(data_path, output_dir=\"preprocessed\"):\n    \n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(\"Loading dataset\")\n    df = pd.read_csv(data_path, sep=\"\\t\")\n    sentences = df[\"sentence\"].dropna().astype(str).tolist()\n    \n    # Normalize all sentences\n    print(\"Normalizing Urdu text\")\n    sentences = [normalize_urdu_text(s) for s in sentences]\n    sentences = [s for s in sentences if len(s.strip()) > 3]\n    print(f\"‚úÖ Total sentences after normalization: {len(sentences)}\")\n    \n    # Create conversation pairs\n    print(\"Creating conversation pairs\")\n    pairs = []\n    for i in range(len(sentences) - 1):\n        src = sentences[i]\n        tgt = sentences[i + 1]\n        if len(src.split()) > 1 and len(tgt.split()) > 1:\n            pairs.append({\"src\": src, \"tgt\": tgt})\n    \n    print(f\"Created {len(pairs)} conversation pairs\")\n    \n    # Save all text for tokenizer training\n    corpus_path = os.path.join(output_dir, \"corpus.txt\")\n    with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n        for text in sentences:\n            f.write(text + \"\\n\")\n    \n    print(f\"Saved corpus to {corpus_path}\")\n    return pairs\n\ndef train_tokenizer(corpus_path, output_prefix, vocab_size=16000):\n    \"\"\"Train SentencePiece tokenizer for Urdu\"\"\"\n    print(f\"Training SentencePiece tokenizer (vocab_size={vocab_size})...\")\n    spm.SentencePieceTrainer.Train(\n        f\"--input={corpus_path} \"\n        f\"--model_prefix={output_prefix} \"\n        f\"--vocab_size={vocab_size} \"\n        f\"--model_type=bpe \"\n        f\"--character_coverage=1.0 \"\n        f\"--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3\"\n    )\n    print(f\"Tokenizer trained: {output_prefix}.model\")\n\ndef prepare_dataset(pairs, sp_model, output_dir):\n    \n    print(\"Loading tokenizer...\")\n    sp = spm.SentencePieceProcessor(model_file=sp_model)\n    \n    # Tokenize pairs\n    print(\"Tokenizing all pairs...\")\n    data = []\n    for pair in tqdm(pairs, desc=\"Tokenizing\"):\n        src_ids = sp.encode(pair[\"src\"], out_type=int)\n        tgt_ids = sp.encode(pair[\"tgt\"], out_type=int)\n        data.append({\n            \"src\": pair[\"src\"],\n            \"tgt\": pair[\"tgt\"],\n            \"src_ids\": src_ids,\n            \"tgt_ids\": tgt_ids\n        })\n    \n    # Shuffle and split (80/10/10)\n    random.shuffle(data)\n    n = len(data)\n    train_idx = int(0.8 * n)\n    val_idx = int(0.9 * n)\n    \n    splits = {\n        \"train\": data[:train_idx],\n        \"val\": data[train_idx:val_idx],\n        \"test\": data[val_idx:]\n    }\n    \n    print(f\"\\nüìä Dataset Split:\")\n    for split_name, split_data in splits.items():\n        path = os.path.join(output_dir, f\"{split_name}.jsonl\")\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            for item in split_data:\n                f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n        print(f\"  - {split_name}: {len(split_data)} samples ({len(split_data)/n*100:.1f}%)\")\n    \n    return splits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.638280Z","iopub.execute_input":"2025-10-27T08:36:27.638823Z","iopub.status.idle":"2025-10-27T08:36:27.648761Z","shell.execute_reply.started":"2025-10-27T08:36:27.638802Z","shell.execute_reply":"2025-10-27T08:36:27.647899Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# DATASET CLASS\n\nclass ConversationDataset(Dataset):\n    \"\"\"PyTorch Dataset for conversation pairs\"\"\"\n    def __init__(self, jsonl_path, max_len=64):\n        self.data = []\n        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                self.data.append(json.loads(line))\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        # Add BOS and EOS tokens\n        src_ids = [2] + item[\"src_ids\"][:self.max_len-2] + [3]\n        tgt_ids = [2] + item[\"tgt_ids\"][:self.max_len-2] + [3]\n        \n        # Pad sequences\n        src_len = len(src_ids)\n        tgt_len = len(tgt_ids)\n        \n        src_ids += [0] * (self.max_len - src_len)\n        tgt_ids += [0] * (self.max_len - tgt_len)\n        \n        return {\n            \"src\": torch.tensor(src_ids[:self.max_len], dtype=torch.long),\n            \"tgt\": torch.tensor(tgt_ids[:self.max_len], dtype=torch.long),\n            \"src_len\": min(src_len, self.max_len),\n            \"tgt_len\": min(tgt_len, self.max_len),\n            \"src_text\": item[\"src\"],\n            \"tgt_text\": item[\"tgt\"]\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.649949Z","iopub.execute_input":"2025-10-27T08:36:27.650292Z","iopub.status.idle":"2025-10-27T08:36:27.669259Z","shell.execute_reply.started":"2025-10-27T08:36:27.650274Z","shell.execute_reply":"2025-10-27T08:36:27.668573Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# TRANSFORMER COMPONENTS\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                            (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def split_heads(self, x):\n        batch_size = x.size(0)\n        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n        return x.transpose(1, 2)\n    \n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n        \n        # Linear projections\n        Q = self.split_heads(self.W_q(query))\n        K = self.split_heads(self.W_k(key))\n        V = self.split_heads(self.W_v(value))\n        \n        # Scaled dot-product attention\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n        \n        context = torch.matmul(attn_weights, V)\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        \n        return self.W_o(context)\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        # Self-attention with residual connection\n        attn_out = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_out))\n        \n        # Feed-forward with residual connection\n        ff_out = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_out))\n        \n        return x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n        # Self-attention with residual connection\n        attn_out = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_out))\n        \n        # Cross-attention with encoder output\n        attn_out = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_out))\n        \n        # Feed-forward with residual connection\n        ff_out = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_out))\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.680567Z","iopub.execute_input":"2025-10-27T08:36:27.681049Z","iopub.status.idle":"2025-10-27T08:36:27.695620Z","shell.execute_reply.started":"2025-10-27T08:36:27.681021Z","shell.execute_reply":"2025-10-27T08:36:27.694800Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# FULL TRANSFORMER MODEL\n\nclass TransformerSeq2Seq(nn.Module):\n    \"\"\"\n    Complete Transformer Encoder-Decoder Model (REQUIREMENT)\n    Built from scratch without pre-trained models\n    \"\"\"\n    def __init__(self, vocab_size, d_model=256, num_heads=4, \n                 num_encoder_layers=2, num_decoder_layers=2, \n                 d_ff=512, dropout=0.1, max_len=512):\n        super().__init__()\n        \n        self.d_model = d_model\n        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n        \n        # Encoder stack\n        self.encoder_layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_encoder_layers)\n        ])\n        \n        # Decoder stack\n        self.decoder_layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_decoder_layers)\n        ])\n        \n        self.output_layer = nn.Linear(d_model, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        \n    def create_padding_mask(self, seq):\n        \"\"\"Create mask for padding tokens\"\"\"\n        return (seq != 0).unsqueeze(1).unsqueeze(2)\n    \n    def create_look_ahead_mask(self, size):\n        \"\"\"Create causal mask for decoder\"\"\"\n        mask = torch.triu(torch.ones(size, size), diagonal=1).bool()\n        return ~mask\n    \n    def encode(self, src):\n        \"\"\"Encoder: Process full input context\"\"\"\n        src_mask = self.create_padding_mask(src)\n        x = self.embedding(src) * math.sqrt(self.d_model)\n        x = self.pos_encoding(x)\n        \n        for layer in self.encoder_layers:\n            x = layer(x, src_mask)\n        \n        return x\n    \n    def decode(self, tgt, enc_output, src_mask=None):\n        \"\"\"Decoder: Generate response token-by-token\"\"\"\n        tgt_mask = self.create_padding_mask(tgt)\n        look_ahead_mask = self.create_look_ahead_mask(tgt.size(1)).to(tgt.device)\n        combined_mask = tgt_mask & look_ahead_mask.unsqueeze(0)\n        \n        x = self.embedding(tgt) * math.sqrt(self.d_model)\n        x = self.pos_encoding(x)\n        \n        for layer in self.decoder_layers:\n            x = layer(x, enc_output, src_mask, combined_mask)\n        \n        return self.output_layer(x)\n    \n    def forward(self, src, tgt):\n        \"\"\"Forward pass for training\"\"\"\n        src_mask = self.create_padding_mask(src)\n        enc_output = self.encode(src)\n        output = self.decode(tgt[:, :-1], enc_output, src_mask)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.728631Z","iopub.execute_input":"2025-10-27T08:36:27.729132Z","iopub.status.idle":"2025-10-27T08:36:27.737632Z","shell.execute_reply.started":"2025-10-27T08:36:27.729112Z","shell.execute_reply":"2025-10-27T08:36:27.736966Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# TRAINING WITH TEACHER FORCING\n\ndef train_model(model, train_loader, val_loader, device, epochs=20, lr=3e-4, \n                teacher_forcing_ratio=0.5):\n    \n    model = model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n    \n    best_val_loss = float('inf')\n    training_history = []\n    \n    print(f\"üöÄ Starting Training (Teacher Forcing Ratio: {teacher_forcing_ratio})\")\n    \n    for epoch in range(epochs):\n        # Training\n        model.train()\n        train_loss = 0\n        \n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        for batch in pbar:\n            src = batch[\"src\"].to(device)\n            tgt = batch[\"tgt\"].to(device)\n            \n            optimizer.zero_grad()\n            \n            # Teacher forcing\n            output = model(src, tgt)\n            \n            # Calculate loss\n            loss = criterion(output.reshape(-1, output.size(-1)), \n                           tgt[:, 1:].reshape(-1))\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            train_loss += loss.item()\n            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n        \n        avg_train_loss = train_loss / len(train_loader)\n        \n        # ===== VALIDATION PHASE =====\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                src = batch[\"src\"].to(device)\n                tgt = batch[\"tgt\"].to(device)\n                output = model(src, tgt)\n                loss = criterion(output.reshape(-1, output.size(-1)), \n                               tgt[:, 1:].reshape(-1))\n                val_loss += loss.item()\n        \n        avg_val_loss = val_loss / len(val_loader)\n        perplexity = math.exp(avg_val_loss)\n        \n        # Log metrics\n        metrics = {\n            'epoch': epoch + 1,\n            'train_loss': avg_train_loss,\n            'val_loss': avg_val_loss,\n            'perplexity': perplexity\n        }\n        training_history.append(metrics)\n        \n        print(f\"\\nüìä Epoch {epoch+1} Results:\")\n        print(f\"   Train Loss: {avg_train_loss:.4f}\")\n        print(f\"   Val Loss: {avg_val_loss:.4f}\")\n        print(f\"   Perplexity: {perplexity:.2f}\")\n        \n        # Save best model\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': avg_val_loss,\n                'perplexity': perplexity,\n                'training_history': training_history\n            }, 'best_model.pth')\n            print(f\"   ‚úÖ Model saved! (Best Val Loss: {avg_val_loss:.4f})\")\n        print()\n    \n    return training_history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.738780Z","iopub.execute_input":"2025-10-27T08:36:27.739027Z","iopub.status.idle":"2025-10-27T08:36:27.755269Z","shell.execute_reply.started":"2025-10-27T08:36:27.739010Z","shell.execute_reply":"2025-10-27T08:36:27.754584Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# INFERENCE FUNCTIONS\n\ndef greedy_decode(model, src, max_len, device, bos_id=2, eos_id=3):\n   \n    model.eval()\n    src = src.to(device)\n    \n    with torch.no_grad():\n        enc_output = model.encode(src)\n        src_mask = model.create_padding_mask(src)\n        \n        tgt = torch.tensor([[bos_id]], dtype=torch.long, device=device)\n        \n        for _ in range(max_len):\n            output = model.decode(tgt, enc_output, src_mask)\n            next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)\n            tgt = torch.cat([tgt, next_token], dim=1)\n            \n            if next_token.item() == eos_id:\n                break\n        \n        return tgt[0].cpu().tolist()\n\ndef beam_search_decode(model, src, beam_size, max_len, device, bos_id=2, eos_id=3):\n    \n    model.eval()\n    src = src.to(device)\n    \n    with torch.no_grad():\n        enc_output = model.encode(src)\n        src_mask = model.create_padding_mask(src)\n        \n        # Initialize beam with BOS token\n        beams = [(torch.tensor([[bos_id]], dtype=torch.long, device=device), 0.0)]\n        completed = []\n        \n        for _ in range(max_len):\n            candidates = []\n            \n            for seq, score in beams:\n                if seq[0, -1].item() == eos_id:\n                    completed.append((seq, score))\n                    continue\n                \n                output = model.decode(seq, enc_output, src_mask)\n                log_probs = F.log_softmax(output[:, -1, :], dim=-1)\n                top_probs, top_indices = log_probs.topk(beam_size)\n                \n                for prob, idx in zip(top_probs[0], top_indices[0]):\n                    new_seq = torch.cat([seq, idx.unsqueeze(0).unsqueeze(0)], dim=1)\n                    new_score = score + prob.item()\n                    candidates.append((new_seq, new_score))\n            \n            if not candidates:\n                break\n            \n            # Select top beam_size candidates\n            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n            \n            if len(completed) >= beam_size:\n                break\n        \n        # Return best completed sequence\n        if completed:\n            best_seq = max(completed, key=lambda x: x[1])[0]\n        else:\n            best_seq = beams[0][0]\n        \n        return best_seq[0].cpu().tolist()\n\ndef generate_response(model, sp, input_text, max_len=64, device='cpu', \n                     strategy='greedy', beam_size=3):\n    \n    # Normalize input\n    input_text = normalize_urdu_text(input_text)\n    \n    # Encode input\n    src_ids = [2] + sp.encode(input_text, out_type=int) + [3]\n    src_tensor = torch.tensor([src_ids], dtype=torch.long)\n    \n    # Generate output\n    if strategy == 'beam':\n        output_ids = beam_search_decode(model, src_tensor, beam_size, max_len, device)\n    else:\n        output_ids = greedy_decode(model, src_tensor, max_len, device)\n    \n    # Decode output\n    response = sp.decode([id for id in output_ids if id not in [0, 2, 3]])\n    \n    return response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.756004Z","iopub.execute_input":"2025-10-27T08:36:27.756289Z","iopub.status.idle":"2025-10-27T08:36:27.774816Z","shell.execute_reply.started":"2025-10-27T08:36:27.756263Z","shell.execute_reply":"2025-10-27T08:36:27.774124Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# EVALUATION METRICS\n\ndef calculate_bleu_score(references, hypotheses):\n    \n    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n    \n    smooth = SmoothingFunction()\n    scores = []\n    \n    for ref, hyp in zip(references, hypotheses):\n        ref_tokens = [ref.split()]\n        hyp_tokens = hyp.split()\n        \n        try:\n            score = sentence_bleu(ref_tokens, hyp_tokens, \n                                smoothing_function=smooth.method1)\n            scores.append(score)\n        except:\n            scores.append(0.0)\n    \n    return np.mean(scores) if scores else 0.0\n\ndef calculate_rouge_l(references, hypotheses):\n    \n    try:\n        from rouge_score import rouge_scorer\n        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n        \n        scores = []\n        for ref, hyp in zip(references, hypotheses):\n            score = scorer.score(ref, hyp)\n            scores.append(score['rougeL'].fmeasure)\n        \n        return np.mean(scores) if scores else 0.0\n    except ImportError:\n        print(\"‚ö†Ô∏è rouge-score not installed. Install: pip install rouge-score\")\n        return 0.0\n\ndef calculate_chrf(references, hypotheses):\n    \n    def char_ngrams(text, n):\n        return [text[i:i+n] for i in range(len(text)-n+1)]\n    \n    scores = []\n    for ref, hyp in zip(references, hypotheses):\n        ref_chars = set(char_ngrams(ref, 3))\n        hyp_chars = set(char_ngrams(hyp, 3))\n        \n        if len(hyp_chars) == 0:\n            scores.append(0.0)\n            continue\n        \n        precision = len(ref_chars & hyp_chars) / len(hyp_chars)\n        recall = len(ref_chars & hyp_chars) / len(ref_chars) if len(ref_chars) > 0 else 0\n        \n        f_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n        scores.append(f_score)\n    \n    return np.mean(scores) if scores else 0.0\n\ndef evaluate_model(model, test_loader, sp, device, num_samples=100):\n    \n    print(\"EVALUATING MODEL\")\n    \n    model.eval()\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n    \n    references = []\n    hypotheses = []\n    total_loss = 0\n    \n    with torch.no_grad():\n        for i, batch in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n            if i >= num_samples // test_loader.batch_size:\n                break\n            \n            src = batch[\"src\"].to(device)\n            tgt = batch[\"tgt\"].to(device)\n            \n            # Calculate loss for perplexity\n            output = model(src, tgt)\n            loss = criterion(output.reshape(-1, output.size(-1)), \n                           tgt[:, 1:].reshape(-1))\n            total_loss += loss.item()\n            \n            # Generate predictions for BLEU/ROUGE/chrF\n            for j in range(min(4, src.size(0))):  # Evaluate 4 per batch\n                src_tensor = src[j:j+1]\n                pred_ids = greedy_decode(model, src_tensor, 64, device)\n                pred_text = sp.decode([id for id in pred_ids if id not in [0, 2, 3]])\n                \n                ref_text = batch[\"tgt_text\"][j]\n                \n                references.append(ref_text)\n                hypotheses.append(pred_text)\n    \n    # Calculate metrics\n    perplexity = math.exp(total_loss / len(test_loader))\n    bleu = calculate_bleu_score(references, hypotheses)\n    rouge_l = calculate_rouge_l(references, hypotheses)\n    chrf = calculate_chrf(references, hypotheses)\n    \n    metrics = {\n        'perplexity': perplexity,\n        'bleu': bleu,\n        'rouge_l': rouge_l,\n        'chrf': chrf\n    }\n    \n    print(f\"\\n{'='*60}\")\n    print(\"üìä EVALUATION RESULTS\")\n    print(f\"{'='*60}\")\n    print(f\"  Perplexity: {perplexity:.2f}\")\n    print(f\"  BLEU Score: {bleu:.4f}\")\n    print(f\"  ROUGE-L: {rouge_l:.4f}\")\n    print(f\"  chrF: {chrf:.4f}\")\n    print(f\"{'='*60}\\n\")\n    \n    # Show qualitative examples\n    print(\"üîç QUALITATIVE EXAMPLES:\\n\")\n    for i in range(min(5, len(references))):\n        print(f\"Input: {references[i][:50]}...\")\n        print(f\"Predicted: {hypotheses[i]}\")\n        print(f\"Reference: {references[i]}\")\n        print(\"-\" * 60)\n    \n    # Save evaluation results\n    with open('evaluation_results.json', 'w', encoding='utf-8') as f:\n        json.dump({\n            'metrics': metrics,\n            'examples': [{'ref': r, 'hyp': h} for r, h in zip(references[:10], hypotheses[:10])]\n        }, f, ensure_ascii=False, indent=2)\n    \n    print(\"‚úÖ Evaluation results saved to 'evaluation_results.json'\\n\")\n    \n    return metrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.775999Z","iopub.execute_input":"2025-10-27T08:36:27.776245Z","iopub.status.idle":"2025-10-27T08:36:27.796575Z","shell.execute_reply.started":"2025-10-27T08:36:27.776229Z","shell.execute_reply":"2025-10-27T08:36:27.795669Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# HUMAN EVALUATION FRAMEWORK\n\ndef conduct_human_evaluation(model, sp, device, test_samples=20):\n\n    evaluation_data = []\n    \n    test_inputs = [\n        \"ÿ¢Ÿæ ⁄©€åÿ≥€í €Å€å⁄∫ÿü\",\n        \"ÿ¢ÿ¨ ŸÖŸàÿ≥ŸÖ ⁄©€åÿ≥ÿß €Å€íÿü\",\n        \"Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ⁄©ÿß ÿØÿßÿ±ÿßŸÑÿ≠⁄©ŸàŸÖÿ™ ⁄©€åÿß €Å€íÿü\",\n        \"ŸÖ€å⁄∫ ÿ¢Ÿæ ⁄©€å ŸÖÿØÿØ ⁄©€åÿ≥€í ⁄©ÿ± ÿ≥⁄©ÿ™ÿß €ÅŸà⁄∫ÿü\",\n        \"ÿ¥⁄©ÿ±€å€Å\"\n    ]\n    \n    for i, input_text in enumerate(test_inputs[:test_samples], 1):\n        print(f\"\\n{'='*60}\")\n        print(f\"Sample {i}/{min(test_samples, len(test_inputs))}\")\n        print(f\"{'='*60}\")\n        \n        response = generate_response(model, sp, input_text, device=device)\n        \n        print(f\"\\nInput: {input_text}\")\n        print(f\"Generated Response: {response}\")\n        print()\n        \n        # In production, collect these from human evaluators\n        # For now, create a template\n        evaluation_data.append({\n            'input': input_text,\n            'response': response,\n            'fluency': None,  # 1-5 score\n            'relevance': None,  # 1-5 score\n            'adequacy': None  # 1-5 score\n        })\n    \n    # Save template for human evaluation\n    with open('human_evaluation_template.json', 'w', encoding='utf-8') as f:\n        json.dump(evaluation_data, f, ensure_ascii=False, indent=2)\n    \n    print(\"\\n‚úÖ Human evaluation template saved to 'human_evaluation_template.json'\")\n    print(\"   Please fill in the fluency, relevance, and adequacy scores (1-5)\")\n    \n    return evaluation_data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.797388Z","iopub.execute_input":"2025-10-27T08:36:27.797580Z","iopub.status.idle":"2025-10-27T08:36:27.815960Z","shell.execute_reply.started":"2025-10-27T08:36:27.797565Z","shell.execute_reply":"2025-10-27T08:36:27.815207Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# MAIN EXECUTION\n\ndef main():\n    # CONFIGURATION\n    CONFIG = {\n        'data_path': \"/kaggle/input/urdu-dataset-20000/final_main_dataset.tsv\",\n        'output_dir': \"preprocessed\",\n        'vocab_size': 16000,\n        'd_model': 256,\n        'num_heads': 4,\n        'num_encoder_layers': 2,\n        'num_decoder_layers': 2,\n        'd_ff': 512,\n        'dropout': 0.2,\n        'batch_size': 32,\n        'epochs': 20,\n        'lr': 3e-4,\n        'max_len': 64,\n        'device': \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    }\n    \n    for key, value in CONFIG.items():\n        print(f\"  {key}: {value}\")\n    \n    print(f\"\\n  Device: {CONFIG['device']}\")\n    if CONFIG['device'] == 'cuda':\n        print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n    print()\n    \n    # PREPROCESSING\n    print(\"STEP 1: DATA PREPROCESSING\")\n    \n    pairs = preprocess_urdu_dataset(CONFIG['data_path'], CONFIG['output_dir'])\n    \n    # TOKENIZER TRAINING\n    print(\"STEP 2: TOKENIZER TRAINING\")\n    \n    train_tokenizer(\n        corpus_path=os.path.join(CONFIG['output_dir'], \"corpus.txt\"),\n        output_prefix=os.path.join(CONFIG['output_dir'], \"sp_urdu\"),\n        vocab_size=CONFIG['vocab_size']\n    )\n    \n    # Dataset Prepare\n    print(\"STEP 3: DATASET PREPARATION\")\n    \n    sp_model_path = os.path.join(CONFIG['output_dir'], \"sp_urdu.model\")\n    prepare_dataset(pairs, sp_model_path, CONFIG['output_dir'])\n    \n    # Data loading\n    print(\"STEP 4: CREATING DATALOADERS\")\n    \n    train_ds = ConversationDataset(\n        os.path.join(CONFIG['output_dir'], \"train.jsonl\"),\n        max_len=CONFIG['max_len']\n    )\n    val_ds = ConversationDataset(\n        os.path.join(CONFIG['output_dir'], \"val.jsonl\"),\n        max_len=CONFIG['max_len']\n    )\n    test_ds = ConversationDataset(\n        os.path.join(CONFIG['output_dir'], \"test.jsonl\"),\n        max_len=CONFIG['max_len']\n    )\n    \n    train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size'])\n    test_loader = DataLoader(test_ds, batch_size=CONFIG['batch_size'])\n    \n    print(f\"  Training batches: {len(train_loader)}\")\n    print(f\"  Validation batches: {len(val_loader)}\")\n    print(f\"  Test batches: {len(test_loader)}\")\n    \n    # Model Initialize\n    print(f\"\\n{'='*60}\")\n    print(\"STEP 5: MODEL INITIALIZATION\")\n    print(f\"{'='*60}\\n\")\n    \n    sp = spm.SentencePieceProcessor(model_file=sp_model_path)\n    vocab_size = sp.get_piece_size()\n    \n    model = TransformerSeq2Seq(\n        vocab_size=vocab_size,\n        d_model=CONFIG['d_model'],\n        num_heads=CONFIG['num_heads'],\n        num_encoder_layers=CONFIG['num_encoder_layers'],\n        num_decoder_layers=CONFIG['num_decoder_layers'],\n        d_ff=CONFIG['d_ff'],\n        dropout=CONFIG['dropout'],\n        max_len=512\n    )\n    \n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    print(f\"  Total Parameters: {total_params:,}\")\n    print(f\"  Trainable Parameters: {trainable_params:,}\")\n    print(f\"  Model Size: {total_params * 4 / 1024 / 1024:.2f} MB (float32)\")\n    \n    # Training\n    print(\"STEP 6: MODEL TRAINING\")\n    \n    training_history = train_model(\n        model, \n        train_loader, \n        val_loader, \n        CONFIG['device'],\n        epochs=CONFIG['epochs'],\n        lr=CONFIG['lr'],\n        teacher_forcing_ratio=0.5\n    )\n    \n    # Save training history\n    with open('training_history.json', 'w') as f:\n        json.dump(training_history, f, indent=2)\n    \n    # LOAD BEST MODEL\n    print(\"STEP 7: LOADING BEST MODEL\")\n    \n    checkpoint = torch.load('best_model.pth', map_location=CONFIG['device'])\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(f\" Loaded best model from epoch {checkpoint['epoch']+1}\")\n    print(f\"   Validation Loss: {checkpoint['val_loss']:.4f}\")\n    print(f\"   Perplexity: {checkpoint['perplexity']:.2f}\")\n    \n    # Evaluation\n    print(\"STEP 8: MODEL EVALUATION\")\n    \n    metrics = evaluate_model(model, test_loader, sp, CONFIG['device'], num_samples=100)\n    \n    # Testing\n    print(\"STEP 9: INTERACTIVE TESTING\")\n    \n    test_inputs = [\n        \"ÿ¢Ÿæ ⁄©ÿß ŸÜÿßŸÖ ⁄©€åÿß €Å€íÿü\",\n        \"ÿ¢ÿ¨ ŸÖŸàÿ≥ŸÖ ⁄©€åÿ≥ÿß €Å€íÿü\",\n        \"Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ⁄©€í ÿ®ÿßÿ±€í ŸÖ€å⁄∫ ÿ®ÿ™ÿßÿ¶€å⁄∫\",\n        \"ÿ¥⁄©ÿ±€å€Å\",\n        \"ÿÆÿØÿß ÿ≠ÿßŸÅÿ∏\",\n        \"ŸÖ€å⁄∫ ÿ¢Ÿæ ⁄©€å ŸÖÿØÿØ ⁄©€åÿ≥€í ⁄©ÿ± ÿ≥⁄©ÿ™ÿß €ÅŸà⁄∫ÿü\"\n    ]\n    \n    print(\"Testing with sample inputs:\\n\")\n    for inp in test_inputs:\n        print(f\"{'='*60}\")\n        print(f\"Input: {inp}\")\n        \n        # Greedy decoding\n        greedy_response = generate_response(\n            model, sp, inp, \n            device=CONFIG['device'], \n            strategy='greedy'\n        )\n        print(f\"Greedy Response: {greedy_response}\")\n        \n        # Beam search decoding\n        beam_response = generate_response(\n            model, sp, inp, \n            device=CONFIG['device'], \n            strategy='beam',\n            beam_size=3\n        )\n        print(f\"Beam Search Response: {beam_response}\")\n        print()\n    \n    # HUMAN EVALUATION SETUP\n    print(\"STEP 10: HUMAN EVALUATION SETUP\")\n    conduct_human_evaluation(model, sp, CONFIG['device'], test_samples=5)\n    \n    # Save Model\n    print(\"SAVING COMPLETE MODEL\")\n    \n    torch.save({\n        'config': CONFIG,\n        'model_state_dict': model.state_dict(),\n        'vocab_size': vocab_size,\n        'training_history': training_history,\n        'evaluation_metrics': metrics\n    }, 'complete_model.pth')\n    \n    # Summary\n\n    print(\"\\n Final Metrics:\")\n    print(f\"   - Perplexity: {metrics['perplexity']:.2f}\")\n    print(f\"   - BLEU: {metrics['bleu']:.4f}\")\n    print(f\"   - ROUGE-L: {metrics['rouge_l']:.4f}\")\n    print(f\"   - chrF: {metrics['chrf']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.816878Z","iopub.execute_input":"2025-10-27T08:36:27.817193Z","iopub.status.idle":"2025-10-27T08:36:27.835706Z","shell.execute_reply.started":"2025-10-27T08:36:27.817146Z","shell.execute_reply":"2025-10-27T08:36:27.835009Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# RUN MAIN\n\nif __name__ == \"__main__\":\n    # Set random seeds for reproducibility\n    random.seed(42)\n    np.random.seed(42)\n    torch.manual_seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n    \n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:36:27.836430Z","iopub.execute_input":"2025-10-27T08:36:27.836638Z","iopub.status.idle":"2025-10-27T08:41:21.999849Z","shell.execute_reply.started":"2025-10-27T08:36:27.836622Z","shell.execute_reply":"2025-10-27T08:41:21.999289Z"}},"outputs":[{"name":"stdout","text":"  data_path: /kaggle/input/urdu-dataset-20000/final_main_dataset.tsv\n  output_dir: preprocessed\n  vocab_size: 16000\n  d_model: 256\n  num_heads: 4\n  num_encoder_layers: 2\n  num_decoder_layers: 2\n  d_ff: 512\n  dropout: 0.2\n  batch_size: 32\n  epochs: 20\n  lr: 0.0003\n  max_len: 64\n  device: cuda\n\n  Device: cuda\n  GPU: Tesla P100-PCIE-16GB\n\nSTEP 1: DATA PREPROCESSING\nLoading dataset\nNormalizing Urdu text\n‚úÖ Total sentences after normalization: 19979\nCreating conversation pairs\nCreated 19310 conversation pairs\nSaved corpus to preprocessed/corpus.txt\nSTEP 2: TOKENIZER TRAINING\nTraining SentencePiece tokenizer (vocab_size=16000)...\nTokenizer trained: preprocessed/sp_urdu.model\nSTEP 3: DATASET PREPARATION\nLoading tokenizer...\nTokenizing all pairs...\n","output_type":"stream"},{"name":"stderr","text":"Tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19310/19310 [00:00<00:00, 20736.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Dataset Split:\n  - train: 15448 samples (80.0%)\n  - val: 1931 samples (10.0%)\n  - test: 1931 samples (10.0%)\nSTEP 4: CREATING DATALOADERS\n  Training batches: 483\n  Validation batches: 61\n  Test batches: 61\n\n============================================================\nSTEP 5: MODEL INITIALIZATION\n============================================================\n\n  Total Parameters: 10,843,776\n  Trainable Parameters: 10,843,776\n  Model Size: 41.37 MB (float32)\nSTEP 6: MODEL TRAINING\nüöÄ Starting Training (Teacher Forcing Ratio: 0.5)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.42it/s, loss=6.2857]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 1 Results:\n   Train Loss: 6.5703\n   Val Loss: 6.1026\n   Perplexity: 447.01\n   ‚úÖ Model saved! (Best Val Loss: 6.1026)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.42it/s, loss=6.1280]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 2 Results:\n   Train Loss: 5.8215\n   Val Loss: 5.7691\n   Perplexity: 320.26\n   ‚úÖ Model saved! (Best Val Loss: 5.7691)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.46it/s, loss=5.5364]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 3 Results:\n   Train Loss: 5.4313\n   Val Loss: 5.5939\n   Perplexity: 268.79\n   ‚úÖ Model saved! (Best Val Loss: 5.5939)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.40it/s, loss=5.4471]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 4 Results:\n   Train Loss: 5.1359\n   Val Loss: 5.3975\n   Perplexity: 220.85\n   ‚úÖ Model saved! (Best Val Loss: 5.3975)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.22it/s, loss=5.0505]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 5 Results:\n   Train Loss: 4.8960\n   Val Loss: 5.2642\n   Perplexity: 193.28\n   ‚úÖ Model saved! (Best Val Loss: 5.2642)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.29it/s, loss=4.5092]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 6 Results:\n   Train Loss: 4.6900\n   Val Loss: 5.1516\n   Perplexity: 172.70\n   ‚úÖ Model saved! (Best Val Loss: 5.1516)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.36it/s, loss=4.5522]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 7 Results:\n   Train Loss: 4.5079\n   Val Loss: 5.0341\n   Perplexity: 153.56\n   ‚úÖ Model saved! (Best Val Loss: 5.0341)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.54it/s, loss=4.6771]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 8 Results:\n   Train Loss: 4.3460\n   Val Loss: 4.9486\n   Perplexity: 140.97\n   ‚úÖ Model saved! (Best Val Loss: 4.9486)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.55it/s, loss=4.1521]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 9 Results:\n   Train Loss: 4.1985\n   Val Loss: 4.8633\n   Perplexity: 129.45\n   ‚úÖ Model saved! (Best Val Loss: 4.8633)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.54it/s, loss=4.1079]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 10 Results:\n   Train Loss: 4.0631\n   Val Loss: 4.7906\n   Perplexity: 120.37\n   ‚úÖ Model saved! (Best Val Loss: 4.7906)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.57it/s, loss=4.0965]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 11 Results:\n   Train Loss: 3.9391\n   Val Loss: 4.7011\n   Perplexity: 110.07\n   ‚úÖ Model saved! (Best Val Loss: 4.7011)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.62it/s, loss=3.8738]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 12 Results:\n   Train Loss: 3.8222\n   Val Loss: 4.6635\n   Perplexity: 106.00\n   ‚úÖ Model saved! (Best Val Loss: 4.6635)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.35it/s, loss=3.9045]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 13 Results:\n   Train Loss: 3.7116\n   Val Loss: 4.6236\n   Perplexity: 101.86\n   ‚úÖ Model saved! (Best Val Loss: 4.6236)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 34.95it/s, loss=3.8084]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 14 Results:\n   Train Loss: 3.6157\n   Val Loss: 4.5562\n   Perplexity: 95.22\n   ‚úÖ Model saved! (Best Val Loss: 4.5562)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.38it/s, loss=3.5970]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 15 Results:\n   Train Loss: 3.5123\n   Val Loss: 4.4819\n   Perplexity: 88.40\n   ‚úÖ Model saved! (Best Val Loss: 4.4819)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.25it/s, loss=3.3389]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 16 Results:\n   Train Loss: 3.4214\n   Val Loss: 4.4707\n   Perplexity: 87.42\n   ‚úÖ Model saved! (Best Val Loss: 4.4707)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.52it/s, loss=3.3299]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 17 Results:\n   Train Loss: 3.3380\n   Val Loss: 4.4177\n   Perplexity: 82.91\n   ‚úÖ Model saved! (Best Val Loss: 4.4177)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.42it/s, loss=3.2765]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 18 Results:\n   Train Loss: 3.2546\n   Val Loss: 4.3806\n   Perplexity: 79.88\n   ‚úÖ Model saved! (Best Val Loss: 4.3806)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.43it/s, loss=3.4271]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 19 Results:\n   Train Loss: 3.1836\n   Val Loss: 4.3564\n   Perplexity: 77.97\n   ‚úÖ Model saved! (Best Val Loss: 4.3564)\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:13<00:00, 35.47it/s, loss=3.3447]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 20 Results:\n   Train Loss: 3.1029\n   Val Loss: 4.3094\n   Perplexity: 74.40\n   ‚úÖ Model saved! (Best Val Loss: 4.3094)\n\nSTEP 7: LOADING BEST MODEL\n Loaded best model from epoch 20\n   Validation Loss: 4.3094\n   Perplexity: 74.40\nSTEP 8: MODEL EVALUATION\nEVALUATING MODEL\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:   5%|‚ñç         | 3/61 [00:00<00:07,  8.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"‚ö†Ô∏è rouge-score not installed. Install: pip install rouge-score\n\n============================================================\nüìä EVALUATION RESULTS\n============================================================\n  Perplexity: 1.24\n  BLEU Score: 0.0053\n  ROUGE-L: 0.0000\n  chrF: 0.0448\n============================================================\n\nüîç QUALITATIVE EXAMPLES:\n\nInput: ⁄©ÿßŸÑ€å ⁄©ÿßŸÅ€å ÿßŸàÿ± ÿ≥⁄Øÿßÿ±Ÿà⁄∫ Ÿæ€Å ⁄Øÿ≤ÿßÿ±ÿß ⁄©ÿ±ÿ™€å€î...\nPredicted: ÿßÿ≥ ⁄©€å ÿÆŸÑÿßŸÅ Ÿàÿ±ÿ≤€å Ÿæÿ± ÿ∫Ÿàÿ± ⁄©€åÿß €Å€åÿü\nReference: ⁄©ÿßŸÑ€å ⁄©ÿßŸÅ€å ÿßŸàÿ± ÿ≥⁄Øÿßÿ±Ÿà⁄∫ Ÿæ€Å ⁄Øÿ≤ÿßÿ±ÿß ⁄©ÿ±ÿ™€å€î\n------------------------------------------------------------\nInput: ⁄©€å ÿ≥ŸÜ Ÿæ€åÿØÿßÿ¶ÿ¥ ⁄©€å ÿ≥ÿ® ÿ≥€å €ÅŸπ ŸÅŸÑŸÖ...\nPredicted: ÿßÿ≥ ÿ≥€å ÿ≤€åÿßÿØ€Å ⁄©€åÿß ÿ¨ÿß ÿ±€Åÿß €Å€åÿü\nReference: ⁄©€å ÿ≥ŸÜ Ÿæ€åÿØÿßÿ¶ÿ¥ ⁄©€å ÿ≥ÿ® ÿ≥€å €ÅŸπ ŸÅŸÑŸÖ\n------------------------------------------------------------\nInput: ÿ¨ÿ® ÿ≥€åÿßÿ≥ÿ™ ÿØÿßŸÜŸà⁄∫ ⁄©€å ÿ≥ÿßÿ™⁄æ ÿπŸàÿßŸÖ ⁄©ÿß ÿπŸÖŸÑ€å ÿ™ÿπŸÑŸÇ €ÅŸà ⁄Øÿß€î...\nPredicted: ŸÖ€å⁄∫ ŸÜ€å ÿßŸæŸÜ€å ÿ∑ÿ±ŸÅ ÿØ€å⁄©⁄æ ŸÑ€åÿß\nReference: ÿ¨ÿ® ÿ≥€åÿßÿ≥ÿ™ ÿØÿßŸÜŸà⁄∫ ⁄©€å ÿ≥ÿßÿ™⁄æ ÿπŸàÿßŸÖ ⁄©ÿß ÿπŸÖŸÑ€å ÿ™ÿπŸÑŸÇ €ÅŸà ⁄Øÿß€î\n------------------------------------------------------------\nInput: €ÅŸÖÿßÿ±€å ÿ™ÿßÿ±€åÿÆ ŸÖ€å⁄∫ ÿßÿ≥ ⁄©€å ÿ®€å ÿ¥ŸÖÿßÿ± ŸÖÿ´ÿßŸÑ€å⁄∫ ŸÖŸà ÿ¨ŸàÿØ €Å€å⁄∫€î...\nPredicted: €å€Å ⁄©Ÿàÿ¶€å ŸÜ€Å€å⁄∫ ⁄©€Åÿß ÿ¨ÿßÿ¶€å ⁄Øÿß ⁄©€Å ŸÅŸÑÿß⁄∫ ÿßÿØŸÖ€å ÿØÿßÿ¶ÿ±€Ç ÿßÿ≥ŸÑÿßŸÖ ÿ≥€å ÿÆÿßÿ±ÿ¨ ⁄©ÿ±ÿ±€Å€å €Å€å⁄∫\nReference: €ÅŸÖÿßÿ±€å ÿ™ÿßÿ±€åÿÆ ŸÖ€å⁄∫ ÿßÿ≥ ⁄©€å ÿ®€å ÿ¥ŸÖÿßÿ± ŸÖÿ´ÿßŸÑ€å⁄∫ ŸÖŸà ÿ¨ŸàÿØ €Å€å⁄∫€î\n------------------------------------------------------------\nInput: €å€Å ÿµŸàÿ±ÿ™ÿ≠ÿßŸÑ ÿØ€å⁄©⁄æ ⁄©ÿ± ⁄©ŸÑ€åÿ¨€Å ŸÖŸÜ€Å ⁄©Ÿà ÿßŸÜ€å ŸÑ⁄Øÿ™ÿß €Å€å...\nPredicted: Ÿà€Å ÿßŸæŸÜ€å ÿ®ŸÜÿØŸà⁄∫ Ÿæÿ± ŸæŸàÿ±€å ÿ∑ÿ±ÿ≠ ÿ≠ÿßŸà€å €Å€å\nReference: €å€Å ÿµŸàÿ±ÿ™ÿ≠ÿßŸÑ ÿØ€å⁄©⁄æ ⁄©ÿ± ⁄©ŸÑ€åÿ¨€Å ŸÖŸÜ€Å ⁄©Ÿà ÿßŸÜ€å ŸÑ⁄Øÿ™ÿß €Å€å\n------------------------------------------------------------\n‚úÖ Evaluation results saved to 'evaluation_results.json'\n\nSTEP 9: INTERACTIVE TESTING\nTesting with sample inputs:\n\n============================================================\nInput: ÿ¢Ÿæ ⁄©ÿß ŸÜÿßŸÖ ⁄©€åÿß €Å€íÿü\nGreedy Response: ÿß€å⁄© ÿßŸàÿ± ÿ≥ŸÜŸÜ€å ŸàÿßŸÑ€å ŸÖÿ≥ŸÑÿ∑ €Å€å€î\nBeam Search Response: ÿßÿ≥ ⁄©ÿß ÿ≠ÿµ€Å €Å€å€î\n\n============================================================\nInput: ÿ¢ÿ¨ ŸÖŸàÿ≥ŸÖ ⁄©€åÿ≥ÿß €Å€íÿü\nGreedy Response: ÿßŸÜ ⁄©€å Ÿàÿ¨€Å ÿ≥€å ÿß€å⁄© €Å€å ÿ™Ÿà ÿßÿ≥ ŸÖ€å⁄∫ ÿ≥ŸàÿßŸÑ ⁄©€å€å ÿ™Ÿà ÿß€å⁄© €Å€å €î\nBeam Search Response: ÿßŸÜ ⁄©€å Ÿàÿ¨€Å ÿ≥€å ÿß€å⁄© €Å€å\n\n============================================================\nInput: Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ⁄©€í ÿ®ÿßÿ±€í ŸÖ€å⁄∫ ÿ®ÿ™ÿßÿ¶€å⁄∫\nGreedy Response: ÿß€å⁄© ÿ®ÿßÿ± Ÿæ⁄æÿ± ŸÖ€åÿ±ÿß ÿßŸÖÿ™ÿ≠ÿßŸÜ ÿ¥ÿ±Ÿàÿπ €ÅŸà ⁄Øÿ¶€å €Å€å⁄∫\nBeam Search Response: ÿ¨€Åÿß⁄∫ ÿØŸà ŸÖÿ™ÿ≠ÿßÿ±ÿ® ÿßŸÅŸàÿßÿ¨ ÿß€å⁄© ÿØŸàÿ≥ÿ±€å ⁄©€å ŸÖÿØ ŸÖŸÇÿßÿ®ŸÑ €ÅŸà⁄∫€î\n\n============================================================\nInput: ÿ¥⁄©ÿ±€å€Å\nGreedy Response: ÿßÿ¨ ÿßŸÜ ⁄©€å ÿ∑ÿ±ŸÅ ÿ≥€å ÿ®⁄ëÿß ÿß⁄Ü⁄æÿß €Å€å\nBeam Search Response: ÿßÿ¨ ÿßŸÜ ⁄©€å ÿ∑ÿ±ŸÅ ÿ≥€å ÿ®⁄ëÿß ÿß⁄Ü⁄æÿß €Å€å\n\n============================================================\nInput: ÿÆÿØÿß ÿ≠ÿßŸÅÿ∏\nGreedy Response: ÿ™Ÿà Ÿæ⁄æÿ± ŸÖŸÅÿßÿØ Ÿæÿ±ÿ≥ÿ™€å ⁄©€å ÿßŸÜÿ™€Åÿß Ÿæÿ≥ŸÜÿØ€å ⁄©ÿß ÿØÿ±ÿ¨€Å ÿ≠ÿ±ÿßÿ±ÿ™\nBeam Search Response: ÿ™Ÿà Ÿæ⁄æÿ± ŸÖŸÅÿßÿØ ⁄©€å ÿßŸÜÿ™€Åÿß €Å€å€î\n\n============================================================\nInput: ŸÖ€å⁄∫ ÿ¢Ÿæ ⁄©€å ŸÖÿØÿØ ⁄©€åÿ≥€í ⁄©ÿ± ÿ≥⁄©ÿ™ÿß €ÅŸà⁄∫ÿü\nGreedy Response: €å€Å ÿ®ÿßÿ™ €Å€å€î\nBeam Search Response: €å€Å ÿ®ÿßÿ™ €Å€å€î\n\nSTEP 10: HUMAN EVALUATION SETUP\n\n============================================================\nSample 1/5\n============================================================\n\nInput: ÿ¢Ÿæ ⁄©€åÿ≥€í €Å€å⁄∫ÿü\nGenerated Response: €å€Å ŸÖŸÇÿØŸÖ€Å ÿßÿÆŸÑÿßŸÇ€å ÿ¥ÿπŸàÿ± ⁄©€å ⁄Øÿß€î\n\n\n============================================================\nSample 2/5\n============================================================\n\nInput: ÿ¢ÿ¨ ŸÖŸàÿ≥ŸÖ ⁄©€åÿ≥ÿß €Å€íÿü\nGenerated Response: ÿßŸÜ ⁄©€å Ÿàÿ¨€Å ÿ≥€å ÿß€å⁄© €Å€å ÿ™Ÿà ÿßÿ≥ ŸÖ€å⁄∫ ÿ≥ŸàÿßŸÑ ⁄©€å€å ÿ™Ÿà ÿß€å⁄© €Å€å €î\n\n\n============================================================\nSample 3/5\n============================================================\n\nInput: Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ⁄©ÿß ÿØÿßÿ±ÿßŸÑÿ≠⁄©ŸàŸÖÿ™ ⁄©€åÿß €Å€íÿü\nGenerated Response: ÿßÿ≥ ÿ≥€å Ÿà€Å ÿßŸÖŸÜ ÿßŸàÿ± ÿßŸÜÿµÿßŸÅ ⁄©ÿß ÿ®⁄ëÿß ⁄Ü€åŸÑŸÜÿ¨ €Å€å€î\n\n\n============================================================\nSample 4/5\n============================================================\n\nInput: ŸÖ€å⁄∫ ÿ¢Ÿæ ⁄©€å ŸÖÿØÿØ ⁄©€åÿ≥€í ⁄©ÿ± ÿ≥⁄©ÿ™ÿß €ÅŸà⁄∫ÿü\nGenerated Response: €å€Å ÿ®ÿßÿ™ €Å€å€î\n\n\n============================================================\nSample 5/5\n============================================================\n\nInput: ÿ¥⁄©ÿ±€å€Å\nGenerated Response: ÿßÿ¨ ÿßŸÜ ⁄©€å ÿ∑ÿ±ŸÅ ÿ≥€å ÿ®⁄ëÿß ÿß⁄Ü⁄æÿß €Å€å\n\n\n‚úÖ Human evaluation template saved to 'human_evaluation_template.json'\n   Please fill in the fluency, relevance, and adequacy scores (1-5)\nSAVING COMPLETE MODEL\n\n Final Metrics:\n   - Perplexity: 1.24\n   - BLEU: 0.0053\n   - ROUGE-L: 0.0000\n   - chrF: 0.0448\n","output_type":"stream"}],"execution_count":26}]}